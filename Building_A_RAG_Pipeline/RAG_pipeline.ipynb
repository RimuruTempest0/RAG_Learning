{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\71032\\anaconda3\\envs\\faiss_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\71032\\anaconda3\\envs\\faiss_env\\lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\71032\\anaconda3\\envs\\faiss_env\\lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\71032\\anaconda3\\envs\\faiss_env\\lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext\n",
    "# import qdrant_client\n",
    "import torch\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from documents and split into smaller chunks\n",
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "Splitter = SentenceSplitter(chunk_size=512)\n",
    "text_chunks = []\n",
    "doc_idxs = []  # maintain relationship with source doc index\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = Splitter.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "\n",
    "    # Store the index of the source document for each text chunk so that each element in the doc_idxs list corresponds to a text chunk in text_chunks.\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "# Create TextNode instances for each chunk and associate metadata\n",
    "# TextNode: store text chunks, can be sentences, paragraph or smaller units.\n",
    "from llama_index.core.schema import TextNode\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(text=text_chunk)\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\71032\\anaconda3\\envs\\faiss_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "# Embed each text chunk using Hugging Face model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(node.get_content(metadata_mode=\"all\"))\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.018322469666600227, -0.02181188017129898, 0.0523066371679306, 0.0056607965379953384, 0.0020047046709805727, 0.055593330413103104, 0.12054598331451416, 0.027539297938346863, 0.05524640902876854, -0.007476648315787315]\n"
     ]
    }
   ],
   "source": [
    "# embedding test\n",
    "embeddings = embed_model.get_text_embedding(\"你好!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=60.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine at 0x277bfb9ff10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10) # chunk_overlap=2 means: \n",
    "# chunk1[this is a test], chunk2[a test for overlap], chunk3[for overlap among chunks]\n",
    "\n",
    "# global\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[text_splitter]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG currently faces two new challenges. Firstly, it struggles with complex data sources integration, having expanded to include various data types such as semi-structured data like tables and structured data like knowledge graphs. Secondly, there is a demand for system interpretability, controllability, and more functional components, making the entire process more flexible but also more complex.\n"
     ]
    }
   ],
   "source": [
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are the RAG current challenges?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform query using query engine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
    "query_str = \"What is Stable Diffusion?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, the current challenges faced by RAG systems include:\n",
      "\n",
      "1. Complex data sources integration: RAG systems can no longer be confined to a single type of unstructured text data source but have expanded to include various data types, such as semi-structured data like tables and structured data like knowledge graphs.\n",
      "2. New demands for system interpretability, controllability, and more functional components: The process is no longer confined to linear but is controlled by multiple control components for retrieval and generation, making the entire system more flexible and complex.\n",
      "\n",
      "These challenges require RAG systems to adapt to new scenarios and incorporate diverse data sources, while also ensuring the system's interpretability, controllability, and functionality.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=7,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.6)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the RAG current challenges?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG currently faces the following new challenges:\n",
      "\n",
      "1. Complex data sources integration. RAG are no longer confined to a single type of unstructured text data source but have expanded to include various data types, such as semi-structured data like tables and structured data like knowledge graphs.\n",
      "2. New demands for system interpretability, controllability, and more functional components.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import KeywordNodePostprocessor\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "\n",
    "node_postprocessors = [\n",
    "    KeywordNodePostprocessor(\n",
    "        required_keywords=[\"RAG\", \"current\", \"challenges\", \"Component\", \"Workflow\"], exclude_keywords=[\"\"]\n",
    "    )\n",
    "]\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever, node_postprocessors=node_postprocessors\n",
    ")\n",
    "response = query_engine.query(\"What are the RAG current challenges?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current challenges in RAG (Retrieval-Augmented Generation) systems include:\n",
      "\n",
      "* Complex data sources integration, requiring the system to handle various data types, such as structured and unstructured data.\n",
      "* New demands for system interpretability, controllability, and maintainability, which are crucial for complex systems like RAG.\n",
      "* Component selection and optimization, as more neural networks are involved in the system, necessitating the choice of appropriate components for specific tasks and resource configurations.\n",
      "* Workflow orchestration and scheduling, ensuring that components are executed in a specific order, processed in parallel under certain conditions, or judged by the LLM based on different outputs.\n",
      "\n",
      "These challenges highlight the need for RAG systems to adapt to diverse application scenarios, increasing the complexity of system design, management, and maintenance.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Please provide a concise and relevant answer to the following query, focusing on the key concepts. Make sure to include information related to \"{include_keyword}\".\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "formatted_prompt = prompt_template.format(\n",
    "    query=\"What are the RAG current challenges?\",\n",
    "    include_keyword=\"current challenges component workflow\",\n",
    ")\n",
    "response = query_engine.query(formatted_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
